apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-down
  namespace: {{ .Values.namespace }}
spec:
  schedule: "{{ .Values.cronjobs.scaleDown.schedule }}"
  suspend: {{ .Values.cronjobs.scaleDown.suspend }}
  jobTemplate:
    spec:
      backoffLimit: {{ .Values.cronjobs.scaleDown.backoffLimit }}
      template:
        spec:
          restartPolicy: Never
          securityContext:
            fsGroup: 0
          serviceAccountName: {{ include "scale-manager.fullname" . }}
          nodeSelector:
            kubernetes.io/hostname: vm167-9fca2980
          volumes:
            - name: data
              persistentVolumeClaim:
                claimName: {{ .Values.pvc.name }}
          containers:
            - name: scaler
              image: bitnami/kubectl:latest
              securityContext:
                runAsUser: 0
                runAsGroup: 0
              volumeMounts:
                - name: data
                  mountPath: /data
              command: ["/bin/bash", "-c"]
              args:
                - |
                  echo "Collecting current scale values in namespace {{ .Values.namespace }} ..."
                  kubectl get deploy,statefulset -n {{ .Values.namespace }} -o json \
                    | jq '.items | map({kind: .kind, name: .metadata.name, replicas: .spec.replicas})' \
                    > /data/scale.json

                  echo "Scaling all Deployments & StatefulSets to 0 replicas..."
                  for obj in $(kubectl get deploy,statefulset -n {{ .Values.namespace }} -o name); do
                    kubectl scale -n {{ .Values.namespace }} $obj --replicas=0
                    echo "Scaled $obj to 0"
                  done

                  echo "Scale-down completed."
