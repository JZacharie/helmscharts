apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-restore
  namespace: {{ .Values.namespace }}
spec:
  schedule: "{{ .Values.cronjobs.scaleRestore.schedule }}"
  suspend: {{ .Values.cronjobs.scaleRestore.suspend }}
  jobTemplate:
    spec:
      backoffLimit: {{ .Values.cronjobs.scaleRestore.backoffLimit }}
      template:
        spec:
          restartPolicy: Never
          securityContext:
            fsGroup: 0
          serviceAccountName: {{ include "scale-manager.fullname" . }}
          nodeSelector:
            kubernetes.io/hostname: vm167-9fca2980
          volumes:
            - name: data
              persistentVolumeClaim:
                claimName: {{ .Values.pvc.name }}
          containers:
            - name: restorer
              image: bitnami/kubectl:latest
              securityContext:
                runAsUser: 0
                runAsGroup: 0
              volumeMounts:
                - name: data
                  mountPath: /data
              command: ["/bin/bash", "-c"]
              args:
                - |
                  echo "Restoring scale values from /data/scale.json ..."
                  cat /data/scale.json | jq -c '.[]' |
                  while read entry; do
                    KIND=$(echo "$entry" | jq -r '.kind')
                    NAME=$(echo "$entry" | jq -r '.name')
                    REPLICAS=$(echo "$entry" | jq -r '.replicas')

                    if kubectl get -n {{ .Values.namespace }} "$KIND/$NAME" >/dev/null 2>&1; then
                      kubectl scale -n {{ .Values.namespace }} "$KIND/$NAME" --replicas="$REPLICAS"
                      echo "Restored $KIND/$NAME to $REPLICAS replicas"
                    fi
                  done

                  echo "Restore completed."
